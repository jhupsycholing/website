Automatically generated by Mendeley Desktop 1.17.10
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{grushacuny2019,
  title={Reassessing the evidence for syntactic adaptation from self-paced reading studies},
  author={Prasad, Grusha and Linzen, Tal},
  journal={Proceedings for The 32nd CUNY Conference on Human Sentence Processing},
  year={2019},
}

@article{alishahi2019analyzing,
  title={Analyzing and Interpreting Neural Networks for NLP: A Report on the First BlackboxNLP Workshop},
  author={Alishahi, Afra and Chrupa{\l}a, Grzegorz and Linzen, Tal},
  journal={Journal of Natural Language Engineering},
  year={2019},
  abstract={The EMNLP 2018 workshop BlackboxNLP was dedicated to resources
and techniques specifically developed for analyzing and understanding the
inner-workings and representations acquired by neural models of language.
Approaches included: systematic manipulation of input to neural networks and investigating the impact on their performance, testing whether
interpretable knowledge can be decoded from intermediate representations
acquired by neural networks, proposing modifications to neural network
architectures to make their knowledge state or generated output more explainable, and examining the performance of networks on simplified or
formal languages. Here we review a number of representative studies in
each category.},
  url={https://arxiv.org/pdf/1904.04063.pdf}
}

@article{prasad2019self,
  title={Do self-paced reading studies provide evidence for rapid syntactic adaptation?},
  author={Prasad, Grusha and Linzen, Tal},
  year={2019},
  publisher={PsyArXiv},
  abstract={Syntactically ambiguous sentences that are disambiguated in favor of a less preferred parse are
read more slowly than their unambiguous counterparts. This is called a garden path effect.
Recent studies have found that this garden path effect decreased as participants are exposed to
many such syntactically ambiguous sentences over the course of an experiment. This decrease
has been interpreted as evidence for rapid syntactic adaptation — i.e. evidence that readers
rapidly calibrate their expectations to a new environment in order to minimize how surprised
they are when they encounter these unexpected syntactic structures (Fine, Jaeger, Farmer, &
Qian, 2013). Syntactic adaptation is only one possible explanation for the observed decrease
in garden-path effect, however: this decrease could also be driven by increased familiarity
with the experimental paradigm (task adaptation), which impacts difficult sentences more than
it does easy ones. The goal of this paper is to tease apart these two explanations. Using a
between-group design, we demonstrate that the decrease in garden path effect is not dependent
on readers’ experience during the experiment. This suggests that it is unlikely to be driven
primarily by syntactic adaptation. We also provide preliminary evidence that the decrease in
garden path effect is driven by asymmetric effects of task adaptation. We conclude that selfpaced reading studies cannot provide unambiguous evidence for rapid syntactic adaptation.},
  url={https://psyarxiv.com/9ptg4/}
}

@article{ravfogel2019studying,
  title={Studying the Inductive Biases of RNNs with Synthetic Variations of Natural Languages},
  author={Ravfogel, Shauli and Goldberg, Yoav and Linzen, Tal},
  journal={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2019},
  abstract={How do typological properties such as word
order and morphological case marking affect the ability of neural sequence models to
acquire the syntax of a language? Crosslinguistic comparisons of RNNs’ syntactic
performance (e.g., on subject-verb agreement
prediction) are complicated by the fact that
any two languages differ in multiple typological properties, as well as by differences in
training corpus. We propose a paradigm that
addresses these issues: we create synthetic
versions of English, which differ from English
in a single typological parameter, and generate
corpora for those languages based on a parsed
English corpus. We report a series of experiments in which RNNs were trained to predict
agreement features for verbs in each of those
synthetic languages. Among other findings,
(1) performance was higher in subject-verbobject order (as in English) than in subjectobject-verb order (as in Japanese), suggesting
that RNNs have a recency bias; (2) predicting agreement with both subject and object
(polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the
two tasks; and (3) overt morphological case
makes agreement prediction significantly easier, regardless of word order.},
  url={https://arxiv.org/pdf/1903.06400}
}

@article{mccoy2019right,
  title={Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
  author={McCoy, R Thomas and Pavlick, Ellie and Linzen, Tal},
  journal={arXiv preprint arXiv:1902.01007},
  year={2019},
  abstract={Machine learning systems can often achieve
high performance on a test set by relying on
heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. Based on an analysis of the task, we
hypothesize three fallible syntactic heuristics
that NLI models are likely to adopt: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine
whether models have adopted these heuristics, we introduce a controlled evaluation set
called HANS (Heuristic Analysis for NLI Systems), which contains many examples where
the heuristics fail. We find that models trained
on MNLI, including the state-of-the-art model
BERT, perform very poorly on HANS, suggesting that they have indeed adopted these
heuristics. We conclude that there is substantial room for improvement in NLI systems,
and that the HANS dataset can motivate and
measure progress in this area.},
  url={https://arxiv.org/pdf/1902.01007.pdf}
}

@inproceedings{kimetal,
  title={Probing what different NLP tasks teach machines about function word comprehension},
  author={Kim, Najoung and Patel, Roma and Poliak, Adam and Wang, Alex and Xia, Patrick and McCoy, R Thomas and Tenney, Ian and Ross, Alexis and Linzen, Tal and Van Durme, Benjamin and Bowman, Samuel R and Pavlick, Ellie},
  journal={Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)},
  year={2019},
}

@inproceedings{lake2019human,
  title={Human few-shot learning of compositional instructions},
  author={Lake, Brenden M and Linzen, Tal and Baroni, Marco},
  journal={Proceedings of the 41st Annual Conference of the Cognitive Science Society},
  year={2019},
  url={https://arxiv.org/pdf/1901.04587},
  abstract={People learn in fast and flexible ways that have not been emulated by machines. Once a person learns a new verb “dax,” he
or she can effortlessly understand how to “dax twice,” “walk
and dax,” or “dax vigorously.” There have been striking recent
improvements in machine learning for natural language processing, yet the best algorithms require vast amounts of experience and struggle to generalize new concepts in compositional
ways. To better understand these distinctively human abilities,
we study the compositional skills of people through languagelike instruction learning tasks. Our results show that people
can learn and use novel functional concepts from very few
examples (few-shot learning), successfully applying familiar
functions to novel inputs. People can also compose concepts
in complex ways that go beyond the provided demonstrations.
Two additional experiments examined the assumptions and inductive biases that people make when solving these tasks, revealing three biases: mutual exclusivity, one-to-one mappings,
and iconic concatenation. We discuss the implications for cognitive modeling and the potential for building machines with
more human-like language learning capabilities.}

}

@inproceedings{grushaetalcuny2019,
  title={Using syntactic priming to investigate how recurrent neural networks represent syntax.},
  author={Prasad, Grusha and van Schijndel, Marten and Linzen, Tal},
  journal={to appear in the Proceedings for The 32nd CUNY Conference on Human Sentence Processing},
  year={2019},
}



@inproceedings{mccoy2018rnns,
  title={RNNs Implicitly Implement Tensor Product Representations},
  author={McCoy, R Thomas and Linzen, Tal and Dunbar, Ewan and Smolensky, Paul},
  journal={To appear in International Conference on Learning Representations (ICLR)},
  year={2019},
  url={https://arxiv.org/abs/1812.08718},
  abstract={Recurrent neural networks (RNNs) can learn continuous vector representations of symbolic structures such as sequences and sentences; these representations often exhibit linear regularities (analogies). Such regularities motivate our hypothesis that RNNs that show such regularities implicitly compile symbolic structures into tensor product representations (TPRs; Smolensky, 1990), which additively combine tensor products of vectors representing roles (e.g., sequence positions) and vectors representing fillers (e.g., particular words). To test this hypothesis, we introduce Tensor Product Decomposition Networks (TPDNs), which use TPRs to approximate existing vector representations. We demonstrate using synthetic data that TPDNs can successfully approximate linear and tree-based RNN autoencoder representations, suggesting that these representations exhibit interpretable compositional structure; we explore the settings that lead RNNs to induce such structure-sensitive representations. By contrast, further TPDN experiments show that the representations of four models trained to encode naturally-occurring sentences can be largely approximated with a bag-of-words, with only marginal improvements from more sophisticated structures. We conclude that TPDNs provide a powerful method for interpreting vector representations, and that standard RNNs can induce compositional sequence representations that are remarkably well approximated by TPRs; at the same time, existing training tasks for sentence representation learning may not be sufficient for inducing robust structural representations.}
}




@article{KingMarantzLinzen2015,
  abstract = {Are syntactic categories like noun and verb categories of stems, such that the noun and verb versions of ambiguous stems like hammer are distinct, though related, lexical items, or are syntactic categories carried by affixes attached to uncategorized roots, such that noun and verb versions of ambiguous stems are derived forms built on a single root? This paper addresses the representational question posed by syntactic categories by examining the processing of category ambiguous words. If syntactic categories are in fact categories of stems, category ambiguity should yield processing uncertainty parallel to that engendered by other forms of lexical ambiguity, such as homophony. On the other hand, if syntactic categories result from affixation, category ambiguity should yield processing uncertainty parallel to that engendered by syntactic uncertainty, at least if morphological structure reduces to syntactic structure as claimed by Distributed Morphology. A magnetoencephalographic (MEG) experiment exploiting a single word lexical decision task supports the syntactic over the lexical account of syntactic categories; category ambiguity parallels syntactic ambiguity rather than lexical ambiguity. The paper illustrates how neurolinguistic data can contribute to testing competing representational theories, but only when tight linking hypotheses are motivated connecting linguistic theory, cognitive processing, and neural responses.},
  title = {Syntactic categories as lexical features or syntactic heads: An MEG approach},
  author = {King, Joseph and Linzen, Tal and Marantz, Alec (accepted with revisions)},
  journal = {Linguistic Inquiry},
  url = {http://tallinzen.net/media/papers/king_linzen_marantz_2015_syntactic_categories.pdf},
  year = {2015}}











